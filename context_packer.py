import os
from pathlib import Path

# --- –ö–û–ù–§–Ü–ì–£–†–ê–¶–Ü–Ø (AI Optimized) ---
OUTPUT_FILE = 'full_project_context.txt'

# –õ—ñ–º—ñ—Ç —Ä–æ–∑–º—ñ—Ä—É –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª—É (—â–æ–± –Ω–µ –∑–∞–±–∏–≤–∞—Ç–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–º—ñ—Ç—Ç—è–º)
MAX_FILE_SIZE_KB = 100  # 100 –ö–ë

# –ü–∞–ø–∫–∏-—ñ–≥–Ω–æ—Ä
IGNORE_DIRS = {
    '.git', 'node_modules', '__pycache__', 'venv', 'env', '.idea', '.vscode', 
    'dist', 'build', 'postgres_data', '.pytest_cache', 'migrations', 
    '.history', 'coverage', 'tmp', 'temp', 'logs', 'assets' # assets —á–∞—Å—Ç–æ –±—ñ–Ω–∞—Ä–Ω—ñ –∞–±–æ –≤–µ–ª–∏–∫—ñ
}

# –§–∞–π–ª–∏-—ñ–≥–Ω–æ—Ä
IGNORE_FILES = {
    'package-lock.json', 'yarn.lock', 'pnpm-lock.yaml', 'poetry.lock', 
    '.DS_Store', 'context_packer.py', OUTPUT_FILE, 
    'debug_db.py', '*.log', '*.sqlite', '*.db', 'favicon.ico'
}

# –†–æ–∑—à–∏—Ä–µ–Ω–Ω—è
ALLOWED_EXTENSIONS = {
    '.py', '.js', '.jsx', '.ts', '.tsx', '.vue', '.html', '.css', '.scss', 
    '.yml', '.yaml', '.json', '.sql', '.dockerfile', '.sh', '.md', '.txt', 
    '.conf', '.ini', '.toml', '.env.example' # –î–æ–¥–∞–Ω–æ .env.example
}

class ContextPacker:
    def __init__(self):
        self.project_root = Path('.')
        self.file_contents = []
        self.tree_structure = []
        self.stats = {'files': 0, 'lines': 0, 'tokens_approx': 0}
        self.extensions_stats = {}

    def should_ignore(self, path):
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ø–∞–ø–æ–∫
        for part in path.parts:
            if part in IGNORE_DIRS:
                return True
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ñ–∞–π–ª—ñ–≤
        if path.name in IGNORE_FILES:
            return True
            
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è
        if path.suffix not in ALLOWED_EXTENSIONS and path.name != 'Dockerfile':
             # –°–ø–µ—Ü—ñ–∞–ª—å–Ω–∏–π –≤–∏–Ω—è—Ç–æ–∫ –¥–ª—è —Ñ–∞–π–ª—ñ–≤ —Ç–∏–ø—É .env.example
            if not path.name.endswith('.example'): 
                return True
            
        return False

    def get_readable_size(self, size_in_bytes):
        for unit in ['B', 'KB', 'MB']:
            if size_in_bytes < 1024:
                return f"{size_in_bytes:.2f} {unit}"
            size_in_bytes /= 1024
        return f"{size_in_bytes:.2f} GB"

    def generate_tree(self):
        # –ì–µ–Ω–µ—Ä—É—î–º–æ –¥–µ—Ä–µ–≤–æ –¥–ª—è –≤—ñ–∑—É–∞–ª—å–Ω–æ–≥–æ —Ä–æ–∑—É–º—ñ–Ω–Ω—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏
        for root, dirs, files in os.walk(self.project_root):
            # –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –ø–∞–ø–æ–∫
            dirs[:] = [d for d in dirs if d not in IGNORE_DIRS]
            
            level = root.replace(str(self.project_root), '').count(os.sep)
            indent = ' ' * 4 * level
            self.tree_structure.append(f"{indent}üìÇ {os.path.basename(root)}/")
            
            subindent = ' ' * 4 * (level + 1)
            for f in files:
                if not self.should_ignore(Path(root) / f):
                    self.tree_structure.append(f"{subindent}üìÑ {f}")

    def scan_files(self):
        for root, dirs, files in os.walk(self.project_root):
            dirs[:] = [d for d in dirs if d not in IGNORE_DIRS]

            for file in files:
                file_path = Path(root) / file
                if self.should_ignore(file_path):
                    continue

                # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ä–æ–∑–º—ñ—Ä—É (Safety fuse)
                file_size_kb = file_path.stat().st_size / 1024
                if file_size_kb > MAX_FILE_SIZE_KB:
                    print(f"‚ö†Ô∏è Skipped large file: {file_path} ({file_size_kb:.2f} KB)")
                    self.file_contents.append(
                        f'<file path="{file_path}">\n'
                        f'\n'
                        f'</file>\n'
                    )
                    continue

                try:
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()
                        lines_count = len(content.splitlines())
                        
                        self.stats['files'] += 1
                        self.stats['lines'] += lines_count
                        self.stats['tokens_approx'] += len(content) // 4
                        
                        ext = file_path.suffix or 'No Ext'
                        self.extensions_stats[ext] = self.extensions_stats.get(ext, 0) + 1

                        # === –û–°–ù–û–í–ù–ê –ó–ú–Ü–ù–ê: XML FORMAT ===
                        # –¶–µ –¥–æ–∑–≤–æ–ª—è—î AI —á—ñ—Ç–∫–æ –±–∞—á–∏—Ç–∏ –º–µ–∂—ñ —Ñ–∞–π–ª—ñ–≤
                        self.file_contents.append(
                            f'\n<file path="{file_path}">\n'
                            f'{content}\n'
                            f'</file>\n'
                        )
                        
                        print(f"‚úÖ Packed: {file_path}")
                except Exception as e:
                    print(f"‚ùå Error reading {file_path}: {e}")

    def generate_ai_header(self):
        """–°—Ç–≤–æ—Ä—é—î 'System Prompt' –¥–ª—è AI –Ω–∞ –ø–æ—á–∞—Ç–∫—É —Ñ–∞–π–ª—É"""
        header = []
        header.append("")
        header.append("")
        header.append("")
        header.append(f"")
        header.append("\n")
        return "\n".join(header)

    def generate_stats_block(self):
        stats_text = []
        stats_text.append("==================================================")
        stats_text.append("üìä PROJECT STATISTICS")
        stats_text.append("==================================================")
        stats_text.append(f"Total Files: {self.stats['files']}")
        stats_text.append(f"Total Lines: {self.stats['lines']}")
        stats_text.append(f"Approx Tokens: ~{self.stats['tokens_approx']}")
        stats_text.append("Extensions:")
        for ext, count in sorted(self.extensions_stats.items(), key=lambda x: x[1], reverse=True):
            stats_text.append(f"  - {ext:<10}: {count}")
        stats_text.append("==================================================\n")
        return "\n".join(stats_text)

    def save(self):
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            # 1. –Ü–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è –¥–ª—è AI
            f.write(self.generate_ai_header())
            
            # 2. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è —Ç–µ–±–µ
            f.write(self.generate_stats_block())
            
            # 3. –î–µ—Ä–µ–≤–æ –ø—Ä–æ–µ–∫—Ç—É (–ö–∞—Ä—Ç–∞)
            f.write("üå≥ PROJECT STRUCTURE\n")
            f.write("==================================================\n")
            f.write("\n".join(self.tree_structure))
            f.write("\n\n")
            
            # 4. –ö–æ–Ω—Ç–µ–Ω—Ç —Ñ–∞–π–ª—ñ–≤
            f.write("üì¶ FILE CONTENTS\n")
            f.write("==================================================\n")
            f.write("".join(self.file_contents))
            
        final_size = Path(OUTPUT_FILE).stat().st_size
        readable_size = self.get_readable_size(final_size)
        
        print("\n" + "="*50)
        print(f"‚úÖ DONE! Context saved to: {OUTPUT_FILE}")
        print(f"üìä Lines: {self.stats['lines']}")
        print(f"‚öñÔ∏è  Size: {readable_size}")

if __name__ == '__main__':
    packer = ContextPacker()
    print("üöÄ Starting AI Context Packer...")
    packer.generate_tree()
    packer.scan_files()
    packer.save()